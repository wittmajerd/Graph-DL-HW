{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XoPrJE5KHg0_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "from dataset import create_QM9_train_val_test_nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dataset size: 130831\n",
            "Train NX graphs: 91581, Train PyG graphs: 91581\n",
            "Validation NX graphs: 19624, Validation PyG graphs: 19624\n",
            "Test NX graphs: 19626, Test PyG graphs: 19626\n"
          ]
        }
      ],
      "source": [
        "train_graphs_subset, val_graphs_subset, test_graphs_subset, \\\n",
        "train_pyg_subset, val_pyg_subset, test_pyg_subset = create_QM9_train_val_test_nx(\n",
        "    subset_size=None,\n",
        "    train_ratio=0.7,\n",
        "    val_ratio=0.15,\n",
        "    test_ratio=0.15,\n",
        "    random_seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train embeddings shape: (700, 32)\n",
            "Validation embeddings shape: (150, 32)\n",
            "Test embeddings shape: (150, 32)\n"
          ]
        }
      ],
      "source": [
        "# Custom Graph2Vec implementation to avoid KarateClub dependency issues\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def custom_graph2vec(nx_graphs, wl_iterations=3, embedding_dim=64):\n",
        "    \"\"\"\n",
        "    Custom Graph2Vec implementation using Weisfeiler-Lehman kernel\n",
        "    \n",
        "    Args:\n",
        "        graphs: List of networkx graphs\n",
        "        wl_iterations: Number of WL iterations\n",
        "        embedding_dim: Dimension of resulting embeddings\n",
        "        \n",
        "    Returns:\n",
        "        numpy array with graph embeddings\n",
        "    \"\"\"\n",
        "        \n",
        "    # Initialize with node labels (use degrees as initial labels)\n",
        "    documents = []\n",
        "    node_labels = [{node: str(nx_graphs[i].degree(node)) for node in nx_graphs[i].nodes()}\n",
        "                    for i in range(len(nx_graphs))]\n",
        "    \n",
        "    # WL iterations\n",
        "    for _ in range(wl_iterations):\n",
        "        # For each graph, create a document of vertex labels\n",
        "        graph_documents = []\n",
        "        new_node_labels = []\n",
        "        \n",
        "        for i, g in enumerate(nx_graphs):\n",
        "            labels = node_labels[i]\n",
        "            new_labels = {}\n",
        "            doc = []\n",
        "            \n",
        "            # For each node, aggregate neighbor labels\n",
        "            for node in g.nodes():\n",
        "                neighbors = sorted([labels[nbr] for nbr in g.neighbors(node)])\n",
        "                new_label = f\"{labels[node]}_{'_'.join(neighbors)}\"\n",
        "                new_labels[node] = new_label\n",
        "                doc.append(new_label)\n",
        "                \n",
        "            new_node_labels.append(new_labels)\n",
        "            graph_documents.append(\" \".join(doc))\n",
        "        \n",
        "        # Update node labels for next iteration\n",
        "        node_labels = new_node_labels\n",
        "        documents.extend(graph_documents)\n",
        "    \n",
        "    # Extract features using bag-of-words approach\n",
        "    vectorizer = CountVectorizer()\n",
        "    bow_matrix = vectorizer.fit_transform(documents)\n",
        "    \n",
        "    # Convert to TF-IDF features\n",
        "    tfidf = TfidfTransformer()\n",
        "    feature_matrix = tfidf.fit_transform(bow_matrix).toarray()\n",
        "    \n",
        "    # Reshape to get graph-level features (each WL iteration adds a document per graph)\n",
        "    num_graphs = len(nx_graphs)\n",
        "    graph_features = np.zeros((num_graphs, feature_matrix.shape[1]))\n",
        "    \n",
        "    for i in range(wl_iterations + 1):\n",
        "        start_idx = i * num_graphs\n",
        "        end_idx = (i + 1) * num_graphs\n",
        "        if start_idx < feature_matrix.shape[0]:\n",
        "            for j in range(num_graphs):\n",
        "                if start_idx + j < feature_matrix.shape[0]:\n",
        "                    graph_features[j] += feature_matrix[start_idx + j]\n",
        "    \n",
        "    # Reduce dimensions if needed\n",
        "    if embedding_dim < graph_features.shape[1]:\n",
        "        pca = PCA(n_components=embedding_dim)\n",
        "        graph_features = pca.fit_transform(graph_features)\n",
        "    \n",
        "    return graph_features\n",
        "\n",
        "train_embeddings = custom_graph2vec(train_graphs_subset, wl_iterations=3, embedding_dim=32)\n",
        "val_embeddings = custom_graph2vec(val_graphs_subset, wl_iterations=3, embedding_dim=32)\n",
        "test_embeddings = custom_graph2vec(test_graphs_subset, wl_iterations=3, embedding_dim=32)\n",
        "\n",
        "print(f'Train embeddings shape: {train_embeddings.shape}')\n",
        "print(f'Validation embeddings shape: {val_embeddings.shape}')\n",
        "print(f'Test embeddings shape: {test_embeddings.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: torch.Size([91581, 8])\n",
            "x_val shape: torch.Size([19624, 8])\n",
            "x_test shape: torch.Size([19626, 8])\n"
          ]
        }
      ],
      "source": [
        "from graph2vec import Graph2Vec # Ensure this path is correct\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize Graph2Vec model\n",
        "g2v_model = Graph2Vec(\n",
        "    wl_iterations=2,\n",
        "    use_node_attribute='x', # Use 'x' for QM9 atom features, or None for degrees\n",
        "    dimensions=8,\n",
        "    workers=4,\n",
        "    down_sampling=0.0001,\n",
        "    epochs=10,\n",
        "    learning_rate=0.025,\n",
        "    min_count=5,\n",
        "    seed=42,\n",
        "    erase_base_features=False,\n",
        ")\n",
        "\n",
        "# Fit the model on the training data (NetworkX graphs)\n",
        "g2v_model.fit(train_graphs_subset)\n",
        "\n",
        "# Get embeddings for the training set\n",
        "train_embeddings = g2v_model.get_embedding()\n",
        "\n",
        "# Infer embeddings for validation and test sets\n",
        "val_embeddings = g2v_model.infer(val_graphs_subset)\n",
        "test_embeddings = g2v_model.infer(test_graphs_subset)\n",
        "\n",
        "# Convert to PyTorch tensors - these are your X data\n",
        "x_train = torch.tensor(train_embeddings, dtype=torch.float32).to(device)\n",
        "x_val = torch.tensor(val_embeddings, dtype=torch.float32).to(device)\n",
        "x_test = torch.tensor(test_embeddings, dtype=torch.float32).to(device)\n",
        "\n",
        "print(f'x_train shape: {x_train.shape}')\n",
        "print(f'x_val shape: {x_val.shape}')\n",
        "print(f'x_test shape: {x_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: torch.Size([91581, 8]), y_train shape: torch.Size([91581, 2])\n",
            "x_val shape: torch.Size([19624, 8]), y_val shape: torch.Size([19624, 2])\n",
            "x_test shape: torch.Size([19626, 8]), y_test shape: torch.Size([19626, 2])\n",
            "x_train device: cuda:0, y_train device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Indices for HOMO and LUMO in QM9 target properties\n",
        "# HOMO is at index 2, LUMO is at index 3 (0-indexed)\n",
        "target_indices = [2, 3]\n",
        "\n",
        "# These are your Y data, selecting only HOMO and LUMO\n",
        "y_train = torch.stack([data.y[0, target_indices] for data in train_pyg_subset]).to(device)\n",
        "y_val = torch.stack([data.y[0, target_indices] for data in val_pyg_subset]).to(device)\n",
        "y_test = torch.stack([data.y[0, target_indices] for data in test_pyg_subset]).to(device)\n",
        "\n",
        "print(f'x_train shape: {x_train.shape}, y_train shape: {y_train.shape}')\n",
        "print(f'x_val shape: {x_val.shape}, y_val shape: {y_val.shape}')\n",
        "print(f'x_test shape: {x_test.shape}, y_test shape: {y_test.shape}')\n",
        "print(f'x_train device: {x_train.device}, y_train device: {y_train.device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class MultiTargetMLP(nn.Module):\n",
        "    def __init__(self, input_dim=15, hidden_dims=[64, 128, 64], output_dim=14):\n",
        "        super(MultiTargetMLP, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        \n",
        "        for dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.BatchNorm1d(dim))\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "            prev_dim = dim\n",
        "            \n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500, Train Loss: 5.2913, Val MSE: 0.8989\n",
            "Epoch 2/500, Train Loss: 1.0552, Val MSE: 0.8427\n",
            "Epoch 3/500, Train Loss: 0.9105, Val MSE: 0.8617\n",
            "Epoch 4/500, Train Loss: 0.8498, Val MSE: 0.8064\n",
            "Epoch 5/500, Train Loss: 0.8265, Val MSE: 0.8744\n",
            "Epoch 6/500, Train Loss: 0.8148, Val MSE: 0.8508\n",
            "Epoch 7/500, Train Loss: 0.8060, Val MSE: 0.9049\n",
            "Epoch 8/500, Train Loss: 0.8002, Val MSE: 0.9170\n",
            "Epoch 9/500, Train Loss: 0.7965, Val MSE: 0.8136\n",
            "Epoch 10/500, Train Loss: 0.7933, Val MSE: 0.8841\n",
            "Epoch 11/500, Train Loss: 0.7868, Val MSE: 0.8127\n",
            "Epoch 12/500, Train Loss: 0.7791, Val MSE: 0.8478\n",
            "Epoch 13/500, Train Loss: 0.7737, Val MSE: 0.8956\n",
            "Epoch 14/500, Train Loss: 0.7681, Val MSE: 0.8049\n",
            "Epoch 15/500, Train Loss: 0.7609, Val MSE: 0.8128\n",
            "Epoch 16/500, Train Loss: 0.7561, Val MSE: 0.8425\n",
            "Epoch 17/500, Train Loss: 0.7535, Val MSE: 0.8334\n",
            "Epoch 18/500, Train Loss: 0.7463, Val MSE: 0.8424\n",
            "Epoch 19/500, Train Loss: 0.7423, Val MSE: 0.8604\n",
            "Epoch 20/500, Train Loss: 0.7371, Val MSE: 0.8561\n",
            "Epoch 21/500, Train Loss: 0.7346, Val MSE: 0.8524\n",
            "Epoch 22/500, Train Loss: 0.7295, Val MSE: 0.8659\n",
            "Epoch 23/500, Train Loss: 0.7262, Val MSE: 0.8172\n",
            "Epoch 24/500, Train Loss: 0.7254, Val MSE: 0.9214\n",
            "Epoch 25/500, Train Loss: 0.7218, Val MSE: 0.8308\n",
            "Epoch 26/500, Train Loss: 0.7194, Val MSE: 0.8554\n",
            "Epoch 27/500, Train Loss: 0.7163, Val MSE: 0.8026\n",
            "Epoch 28/500, Train Loss: 0.7146, Val MSE: 0.8229\n",
            "Epoch 29/500, Train Loss: 0.7126, Val MSE: 0.8637\n",
            "Epoch 30/500, Train Loss: 0.7111, Val MSE: 0.8556\n",
            "Epoch 31/500, Train Loss: 0.7083, Val MSE: 0.8540\n",
            "Epoch 32/500, Train Loss: 0.7067, Val MSE: 0.8820\n",
            "Epoch 33/500, Train Loss: 0.7058, Val MSE: 0.8169\n",
            "Epoch 34/500, Train Loss: 0.7040, Val MSE: 0.8178\n",
            "Epoch 35/500, Train Loss: 0.7035, Val MSE: 0.8771\n",
            "Epoch 36/500, Train Loss: 0.7000, Val MSE: 0.8193\n",
            "Epoch 37/500, Train Loss: 0.7011, Val MSE: 0.8159\n",
            "Epoch 38/500, Train Loss: 0.6986, Val MSE: 0.8078\n",
            "Epoch 39/500, Train Loss: 0.6985, Val MSE: 0.8240\n",
            "Epoch 40/500, Train Loss: 0.6988, Val MSE: 0.8197\n",
            "Epoch 41/500, Train Loss: 0.6966, Val MSE: 0.8198\n",
            "Epoch 42/500, Train Loss: 0.6955, Val MSE: 0.8285\n",
            "Epoch 43/500, Train Loss: 0.6952, Val MSE: 0.7935\n",
            "Epoch 44/500, Train Loss: 0.6951, Val MSE: 0.8500\n",
            "Epoch 45/500, Train Loss: 0.6947, Val MSE: 0.8270\n",
            "Epoch 46/500, Train Loss: 0.6939, Val MSE: 0.8433\n",
            "Epoch 47/500, Train Loss: 0.6948, Val MSE: 0.8394\n",
            "Epoch 48/500, Train Loss: 0.6933, Val MSE: 0.8753\n",
            "Epoch 49/500, Train Loss: 0.6925, Val MSE: 0.8314\n",
            "Epoch 50/500, Train Loss: 0.6921, Val MSE: 0.8770\n",
            "Epoch 51/500, Train Loss: 0.6921, Val MSE: 0.8636\n",
            "Epoch 52/500, Train Loss: 0.6907, Val MSE: 0.8709\n",
            "Epoch 53/500, Train Loss: 0.6917, Val MSE: 0.8195\n",
            "Epoch 54/500, Train Loss: 0.6915, Val MSE: 0.8287\n",
            "Epoch 55/500, Train Loss: 0.6912, Val MSE: 0.8260\n",
            "Epoch 56/500, Train Loss: 0.6898, Val MSE: 0.7982\n",
            "Epoch 57/500, Train Loss: 0.6908, Val MSE: 0.8394\n",
            "Epoch 58/500, Train Loss: 0.6900, Val MSE: 0.8041\n",
            "Epoch 59/500, Train Loss: 0.6897, Val MSE: 0.8311\n",
            "Epoch 60/500, Train Loss: 0.6892, Val MSE: 0.8206\n",
            "Epoch 61/500, Train Loss: 0.6885, Val MSE: 0.8663\n",
            "Epoch 62/500, Train Loss: 0.6887, Val MSE: 0.8360\n",
            "Epoch 63/500, Train Loss: 0.6874, Val MSE: 0.7894\n",
            "Epoch 64/500, Train Loss: 0.6883, Val MSE: 0.8398\n",
            "Epoch 65/500, Train Loss: 0.6868, Val MSE: 0.8053\n",
            "Epoch 66/500, Train Loss: 0.6878, Val MSE: 0.8289\n",
            "Epoch 67/500, Train Loss: 0.6890, Val MSE: 0.7910\n",
            "Epoch 68/500, Train Loss: 0.6887, Val MSE: 0.8361\n",
            "Epoch 69/500, Train Loss: 0.6871, Val MSE: 0.8090\n",
            "Epoch 70/500, Train Loss: 0.6878, Val MSE: 0.7922\n",
            "Epoch 71/500, Train Loss: 0.6865, Val MSE: 0.8258\n",
            "Epoch 72/500, Train Loss: 0.6874, Val MSE: 0.8207\n",
            "Epoch 73/500, Train Loss: 0.6885, Val MSE: 0.8228\n",
            "Epoch 74/500, Train Loss: 0.6858, Val MSE: 0.8320\n",
            "Epoch 75/500, Train Loss: 0.6858, Val MSE: 0.7920\n",
            "Epoch 76/500, Train Loss: 0.6871, Val MSE: 0.8295\n",
            "Epoch 77/500, Train Loss: 0.6863, Val MSE: 0.7934\n",
            "Epoch 78/500, Train Loss: 0.6856, Val MSE: 0.8133\n",
            "Epoch 79/500, Train Loss: 0.6866, Val MSE: 0.8287\n",
            "Epoch 80/500, Train Loss: 0.6855, Val MSE: 0.8061\n",
            "Epoch 81/500, Train Loss: 0.6859, Val MSE: 0.8343\n",
            "Epoch 82/500, Train Loss: 0.6854, Val MSE: 0.8245\n",
            "Epoch 83/500, Train Loss: 0.6857, Val MSE: 0.8133\n",
            "Epoch 84/500, Train Loss: 0.6849, Val MSE: 0.8194\n",
            "Epoch 85/500, Train Loss: 0.6852, Val MSE: 0.8178\n",
            "Epoch 86/500, Train Loss: 0.6845, Val MSE: 0.8101\n",
            "Epoch 87/500, Train Loss: 0.6849, Val MSE: 0.8337\n",
            "Epoch 88/500, Train Loss: 0.6859, Val MSE: 0.8039\n",
            "Epoch 89/500, Train Loss: 0.6867, Val MSE: 0.8148\n",
            "Epoch 90/500, Train Loss: 0.6850, Val MSE: 0.8388\n",
            "Epoch 91/500, Train Loss: 0.6861, Val MSE: 0.8313\n",
            "Epoch 92/500, Train Loss: 0.6848, Val MSE: 0.8302\n",
            "Epoch 93/500, Train Loss: 0.6849, Val MSE: 0.8342\n",
            "Epoch 94/500, Train Loss: 0.6847, Val MSE: 0.8255\n",
            "Epoch 95/500, Train Loss: 0.6846, Val MSE: 0.8563\n",
            "Epoch 96/500, Train Loss: 0.6837, Val MSE: 0.8110\n",
            "Epoch 97/500, Train Loss: 0.6834, Val MSE: 0.8019\n",
            "Epoch 98/500, Train Loss: 0.6851, Val MSE: 0.8256\n",
            "Epoch 99/500, Train Loss: 0.6834, Val MSE: 0.8496\n",
            "Epoch 100/500, Train Loss: 0.6860, Val MSE: 0.8164\n",
            "Epoch 101/500, Train Loss: 0.6848, Val MSE: 0.8214\n",
            "Epoch 102/500, Train Loss: 0.6854, Val MSE: 0.8404\n",
            "Epoch 103/500, Train Loss: 0.6839, Val MSE: 0.8294\n",
            "Epoch 104/500, Train Loss: 0.6836, Val MSE: 0.8565\n",
            "Epoch 105/500, Train Loss: 0.6844, Val MSE: 0.8404\n",
            "Epoch 106/500, Train Loss: 0.6841, Val MSE: 0.8165\n",
            "Epoch 107/500, Train Loss: 0.6844, Val MSE: 0.7929\n",
            "Epoch 108/500, Train Loss: 0.6817, Val MSE: 0.8051\n",
            "Epoch 109/500, Train Loss: 0.6830, Val MSE: 0.8135\n",
            "Epoch 110/500, Train Loss: 0.6839, Val MSE: 0.8316\n",
            "Epoch 111/500, Train Loss: 0.6831, Val MSE: 0.8418\n",
            "Epoch 112/500, Train Loss: 0.6827, Val MSE: 0.8120\n",
            "Epoch 113/500, Train Loss: 0.6827, Val MSE: 0.8160\n",
            "Early stopping at epoch 113\n",
            "Test MSE with MLP: 0.7784\n"
          ]
        }
      ],
      "source": [
        "# Create dataset and dataloader\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = MultiTargetMLP(input_dim=x_train.shape[1], output_dim=y_train.shape[1]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 500\n",
        "best_val_mse = float('inf')\n",
        "patience = 50\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for X_batch, y_batch in train_loader:        \n",
        "        # Forward pass\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(x_val)\n",
        "        val_loss = criterion(val_outputs, y_val).item()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val MSE: {val_loss:.4f}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_loss < best_val_mse:\n",
        "        best_val_mse = val_loss\n",
        "        counter = 0\n",
        "        # Save model\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# Load best model and evaluate on test set\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_preds = model(x_test).cpu().numpy()\n",
        "    test_mse = mean_squared_error(y_test.cpu().numpy(), test_preds)\n",
        "    print(f\"Test MSE with MLP: {test_mse:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "graph_ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
