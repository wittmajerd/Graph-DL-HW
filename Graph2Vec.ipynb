{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoPrJE5KHg0_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "from dataset import create_QM9_nx_datasets\n",
        "from graph2vec import Graph2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dataset size: 130831\n",
            "Train NX graphs: 104664, Train PyG graphs: 104664\n",
            "Validation NX graphs: 13083, Validation PyG graphs: 13083\n",
            "Test NX graphs: 13084, Test PyG graphs: 13084\n"
          ]
        }
      ],
      "source": [
        "train_graphs_subset, val_graphs_subset, test_graphs_subset, \\\n",
        "train_pyg_subset, val_pyg_subset, test_pyg_subset = create_QM9_nx_datasets(\n",
        "    subset_size=None,\n",
        "    train_ratio=0.8,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    random_seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: torch.Size([104664, 128])\n",
            "x_val shape: torch.Size([13083, 128])\n",
            "x_test shape: torch.Size([13084, 128])\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize Graph2Vec model\n",
        "g2v_model = Graph2Vec(\n",
        "    wl_iterations=3,\n",
        "    use_node_attribute='x', # Use 'x' for QM9 atom features, or None for degrees\n",
        "    dimensions=128,\n",
        "    workers=4,\n",
        "    down_sampling=0.01,\n",
        "    epochs=10,\n",
        "    learning_rate=0.1,\n",
        "    min_count=5,\n",
        "    seed=42,\n",
        "    erase_base_features=False,\n",
        ")\n",
        "\n",
        "# Fit the model on the training data (NetworkX graphs)\n",
        "g2v_model.fit(train_graphs_subset)\n",
        "\n",
        "# Get embeddings for the training set\n",
        "train_embeddings = g2v_model.get_embedding()\n",
        "\n",
        "# Infer embeddings for validation and test sets\n",
        "val_embeddings = g2v_model.infer(val_graphs_subset)\n",
        "test_embeddings = g2v_model.infer(test_graphs_subset)\n",
        "\n",
        "# Convert to PyTorch tensors - these are your X data\n",
        "x_train = torch.tensor(train_embeddings, dtype=torch.float32).to(device)\n",
        "x_val = torch.tensor(val_embeddings, dtype=torch.float32).to(device)\n",
        "x_test = torch.tensor(test_embeddings, dtype=torch.float32).to(device)\n",
        "\n",
        "print(f'x_train shape: {x_train.shape}')\n",
        "print(f'x_val shape: {x_val.shape}')\n",
        "print(f'x_test shape: {x_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: torch.Size([104664, 128]), y_train shape: torch.Size([104664, 5])\n",
            "x_val shape: torch.Size([13083, 128]), y_val shape: torch.Size([13083, 5])\n",
            "x_test shape: torch.Size([13084, 128]), y_test shape: torch.Size([13084, 5])\n",
            "x_train device: cuda:0, y_train device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Indices for HOMO and LUMO in QM9 target properties\n",
        "# HOMO is at index 2, LUMO is at index 3 (0-indexed)\n",
        "# target_indices = [2, 3]\n",
        "target_indices = [0, 1, 2, 3, 4]\n",
        "\n",
        "# These are your Y data, selecting only HOMO and LUMO\n",
        "y_train = torch.stack([data.y[0, target_indices] for data in train_pyg_subset]).to(device)\n",
        "y_val = torch.stack([data.y[0, target_indices] for data in val_pyg_subset]).to(device)\n",
        "y_test = torch.stack([data.y[0, target_indices] for data in test_pyg_subset]).to(device)\n",
        "\n",
        "print(f'x_train shape: {x_train.shape}, y_train shape: {y_train.shape}')\n",
        "print(f'x_val shape: {x_val.shape}, y_val shape: {y_val.shape}')\n",
        "print(f'x_test shape: {x_test.shape}, y_test shape: {y_test.shape}')\n",
        "print(f'x_train device: {x_train.device}, y_train device: {y_train.device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class MultiTargetMLP(nn.Module):\n",
        "    def __init__(self, input_dim=15, hidden_dims=[128, 256, 64], output_dim=14):\n",
        "        super(MultiTargetMLP, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        \n",
        "        for dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.BatchNorm1d(dim))\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "            prev_dim = dim\n",
        "            \n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200, Train Loss: 939.6244, Val MSE: 568.0020\n",
            "Epoch 2/200, Train Loss: 235.8558, Val MSE: 42.9129\n",
            "Epoch 3/200, Train Loss: 20.3711, Val MSE: 6.4702\n",
            "Epoch 4/200, Train Loss: 13.5209, Val MSE: 6.0164\n",
            "Epoch 5/200, Train Loss: 12.6921, Val MSE: 5.7453\n",
            "Epoch 6/200, Train Loss: 12.1297, Val MSE: 5.7582\n",
            "Epoch 7/200, Train Loss: 11.5151, Val MSE: 5.6403\n",
            "Epoch 8/200, Train Loss: 11.1881, Val MSE: 5.4031\n",
            "Epoch 9/200, Train Loss: 10.8372, Val MSE: 5.2643\n",
            "Epoch 10/200, Train Loss: 10.6541, Val MSE: 5.2320\n",
            "Epoch 11/200, Train Loss: 10.4700, Val MSE: 5.1925\n",
            "Epoch 12/200, Train Loss: 10.3535, Val MSE: 5.1217\n",
            "Epoch 13/200, Train Loss: 10.2968, Val MSE: 4.9268\n",
            "Epoch 14/200, Train Loss: 10.1172, Val MSE: 4.9956\n",
            "Epoch 15/200, Train Loss: 9.9976, Val MSE: 4.8073\n",
            "Epoch 16/200, Train Loss: 9.9168, Val MSE: 4.8590\n",
            "Epoch 17/200, Train Loss: 9.8378, Val MSE: 4.8664\n",
            "Epoch 18/200, Train Loss: 9.8298, Val MSE: 4.7012\n",
            "Epoch 19/200, Train Loss: 9.7220, Val MSE: 4.7595\n",
            "Epoch 20/200, Train Loss: 9.6974, Val MSE: 4.6965\n",
            "Epoch 21/200, Train Loss: 9.6180, Val MSE: 4.6183\n",
            "Epoch 22/200, Train Loss: 9.5717, Val MSE: 4.6543\n",
            "Epoch 23/200, Train Loss: 9.4860, Val MSE: 4.7410\n",
            "Epoch 24/200, Train Loss: 9.4759, Val MSE: 4.6464\n",
            "Epoch 25/200, Train Loss: 9.4106, Val MSE: 4.6503\n",
            "Epoch 26/200, Train Loss: 9.3608, Val MSE: 4.6078\n",
            "Epoch 27/200, Train Loss: 9.2754, Val MSE: 4.5328\n",
            "Epoch 28/200, Train Loss: 9.1740, Val MSE: 4.5144\n",
            "Epoch 29/200, Train Loss: 9.2179, Val MSE: 4.6620\n",
            "Epoch 30/200, Train Loss: 9.1033, Val MSE: 4.4024\n",
            "Epoch 31/200, Train Loss: 9.1432, Val MSE: 4.5120\n",
            "Epoch 32/200, Train Loss: 9.0787, Val MSE: 4.4733\n",
            "Epoch 33/200, Train Loss: 9.1132, Val MSE: 4.4052\n",
            "Epoch 34/200, Train Loss: 9.0470, Val MSE: 4.4859\n",
            "Epoch 35/200, Train Loss: 8.9948, Val MSE: 4.4839\n",
            "Epoch 36/200, Train Loss: 8.9873, Val MSE: 4.4408\n",
            "Epoch 37/200, Train Loss: 8.8541, Val MSE: 4.6056\n",
            "Epoch 38/200, Train Loss: 8.8413, Val MSE: 4.4991\n",
            "Epoch 39/200, Train Loss: 8.8831, Val MSE: 4.4217\n",
            "Epoch 40/200, Train Loss: 8.7693, Val MSE: 4.5573\n",
            "Early stopping at epoch 40\n",
            "Test MSE with MLP: 4.4426\n"
          ]
        }
      ],
      "source": [
        "# Create dataset and dataloader\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = MultiTargetMLP(input_dim=x_train.shape[1], output_dim=y_train.shape[1]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 200\n",
        "best_val_mse = float('inf')\n",
        "patience = 10\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for X_batch, y_batch in train_loader:        \n",
        "        # Forward pass\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(x_val)\n",
        "        val_loss = criterion(val_outputs, y_val).item()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val MSE: {val_loss:.4f}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_loss < best_val_mse:\n",
        "        best_val_mse = val_loss\n",
        "        counter = 0\n",
        "        # Save model\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# Load best model and evaluate on test set\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_preds = model(x_test).cpu().numpy()\n",
        "    test_mse = mean_squared_error(y_test.cpu().numpy(), test_preds)\n",
        "    print(f\"Test MSE with MLP: {test_mse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "HOMO LUMO  \n",
        "embedding optim  \n",
        "dim 16, lr 0.025 - Test MSE with MLP: 0.6305    - wl iter 2, downsampling 0.0001  \n",
        "dim 16, lr 0.05  - Test MSE with MLP: 0.3456    - wl iter 2, downsampling 0.0001  \n",
        "dim 16, lr 0.1   - Test MSE with MLP: 0.3108    - wl iter 2, downsampling 0.0001  \n",
        "dim 16, lr 0.5   - Test MSE with MLP: 1.0037    - wl iter 2, downsampling 0.0001  \n",
        "dim 32, lr 0.1   - Test MSE with MLP: 0.2033    - wl iter 2, downsampling 0.0001  \n",
        "dim 64, lr 0.1   - Test MSE with MLP: 0.1669    - wl iter 2, downsampling 0.0001  \n",
        "dim 128, lr 0.1  - Test MSE with MLP: 0.1251    - bigger MLP & wl iter 2, downsampling 0.0001  \n",
        "dim 128, lr 0.1  - Test MSE with MLP: 0.1004    - bigger MLP & wl iter 3, downsampling 0.0001  \n",
        "dim 128, lr 0.1  - Test MSE with MLP: 0.0801    - bigger MLP & wl iter 3, downsampling 0.01  \n",
        "dim 128, lr 0.1  - Test MSE with MLP: 0.0827    - bigger MLP & wl iter 3, downsampling 0.1  \n",
        "\n",
        "mlp lr\n",
        "dim 128, lr 0.1  - Test MSE with MLP: 0.1141    but less epoch     - bigger MLP with 0.01 lr instead of 0.001 & wl iter 3 instead of 2, downsampling 0.01  \n",
        "dim 128, lr 0.1  - Test MSE with MLP: 0.0966                       - bigger MLP with 0.005 lr & wl iter 3 instead of 2, downsampling 0.01  \n",
        "BEST  \n",
        "dim 128, lr 0.1  - Test MSE with MLP: 0.0798                       - bigger MLP with 0.0005 lr & wl iter 3 instead of 2, downsampling 0.01  \n",
        "\n",
        "First 5 property with the same settings  \n",
        "Test MSE with MLP: 4.4426  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "graph_ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
